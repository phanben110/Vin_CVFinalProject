{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJqlGdzZKorr"
      },
      "source": [
        "# Dependencies \n",
        "\n",
        "github: https://github.com/mahmoodlab/HIPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opE8Aai_Kp6-"
      },
      "outputs": [],
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgedqbSsNweq"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import glob\n",
        "from itertools import chain\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucExu6dfLxac"
      },
      "outputs": [],
      "source": [
        "%cd /content/gdrive/MyDrive/Ben/HIPT_4K\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oho1gBIdKorx"
      },
      "source": [
        "# Standalone HIPT_4K Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAKZVyV-Kory"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgQVtI3pNeIX"
      },
      "outputs": [],
      "source": [
        "# Training settings\n",
        "batch_size = 1\n",
        "epochs = 10\n",
        "lr = 3e-5\n",
        "gamma = 0.\n",
        "\n",
        "seed = 42\n",
        "device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y43AmRePNkjv"
      },
      "outputs": [],
      "source": [
        "train_dir = '/content/gdrive/MyDrive/Ben/HIPT_4K/image'\n",
        "#train_dir = '/content/gdrive/MyDrive/Ben/image58'\n",
        "# test_dir = '/content/gdrive/MyDrive/VinBigData/FinalProjectCV/archive/test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI_uju7hNktE"
      },
      "outputs": [],
      "source": [
        "train_list = glob.glob(os.path.join(train_dir,'*.png'))\n",
        "# test_list = glob.glob(os.path.join(test_dir, '*.jpg'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QjVoj1BNvOS"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f42aaPjgNkx1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "train = pd.read_csv('/content/gdrive/MyDrive/Ben/HIPT_4K/train.csv')\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObnqdbmDNk0i"
      },
      "outputs": [],
      "source": [
        "labels = [path.split('/')[-1].split('.')[0] for path in train_list] \n",
        "Y = []\n",
        "for label in labels: \n",
        "    y = train.loc[train['image_id'] == label ]['label'].values[0]\n",
        "    Y.append(1) if y == \"CE\" else Y.append(0)\n",
        "\n",
        "    \n",
        "labels = np.array(Y) \n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfqqfJzlOFQk"
      },
      "outputs": [],
      "source": [
        "random_idx = np.random.randint(1, len(train_list), size=9)\n",
        "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
        "\n",
        "for idx, ax in enumerate(axes.ravel()):\n",
        "    img = Image.open(train_list[idx])\n",
        "    ax.set_title(labels[idx])\n",
        "    ax.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjFIqyEfOFTi"
      },
      "outputs": [],
      "source": [
        "train_list, valid_list = train_test_split(train_list, \n",
        "                                          test_size=0.2,\n",
        "                                          stratify=labels,\n",
        "                                          random_state=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFbML1vWOFWr"
      },
      "outputs": [],
      "source": [
        "print(f\"Train Data: {len(train_list)}\")\n",
        "print(f\"Validation Data: {len(valid_list)}\")\n",
        "# print(f\"Test Data: {len(test_list)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U01SXIC-OFaD"
      },
      "outputs": [],
      "source": [
        "resize = 224\n",
        "train_transforms = transforms.Compose(\n",
        "    [\n",
        "        #transforms.Resize((resize, resize)),\n",
        "        #transforms.RandomResizedCrop(resize),\n",
        "        #transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transforms = transforms.Compose(\n",
        "    [\n",
        "        #transforms.Resize(resize),\n",
        "        #transforms.CenterCrop(resize),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# test_transforms = transforms.Compose(\n",
        "#     [\n",
        "#         transforms.Resize(resize),\n",
        "#         transforms.CenterCrop(resize),\n",
        "#         transforms.ToTensor(),\n",
        "#     ]\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnHtdI_gOFgG"
      },
      "outputs": [],
      "source": [
        "class HIPTDataset(Dataset):\n",
        "    def __init__(self, file_list, transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        self.filelength = len(self.file_list)\n",
        "        return self.filelength\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.file_list[idx]\n",
        "        img = Image.open(img_path)\n",
        "        img_transformed = self.transform(img)\n",
        "\n",
        "        label = img_path.split(\"/\")[-1].split(\".\")[0]\n",
        "        label = train.loc[train['image_id'] == label]['label'].values[0]\n",
        "        label = 1 if label == \"CE\" else 0\n",
        "\n",
        "        return img_transformed, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qie1fCxYOFhz"
      },
      "outputs": [],
      "source": [
        "train_data = HIPTDataset(train_list, transform=train_transforms)\n",
        "valid_data = HIPTDataset(valid_list, transform=val_transforms)\n",
        "# test_data = CatsDogsDataset(test_list, transform=test_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ4J8idnOFi1"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(dataset = train_data, batch_size=batch_size, shuffle=True , num_workers =4)\n",
        "valid_loader = DataLoader(dataset = valid_data, batch_size=batch_size, shuffle=True , num_workers =4)\n",
        "# test_loader = DataLoader(dataset = test_data, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgXrni-HOeK4"
      },
      "outputs": [],
      "source": [
        "print(len(train_data), len(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dj-3KAF4OeNg"
      },
      "outputs": [],
      "source": [
        "print(len(valid_data), len(valid_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkj_sEq2OeQY"
      },
      "outputs": [],
      "source": [
        "from hipt_4k import HIPT_4K\n",
        "from hipt_model_utils import get_vit256, get_vit4k, eval_transforms\n",
        "from hipt_heatmap_utils import *\n",
        "light_jet = cmap_map(lambda x: x/2 + 0.5, matplotlib.cm.jet)\n",
        "\n",
        "pretrained_weights256 = '../Checkpoints/vit256_small_dino.pth'\n",
        "pretrained_weights4k = '../Checkpoints/vit4k_xs_dino.pth'\n",
        "device256 = torch.device(\"cpu\")\n",
        "device4k = torch.device(\"cpu\")\n",
        "\n",
        "### ViT_256 + ViT_4K loaded independently (used for Attention Heatmaps)\n",
        "model256 = get_vit256(pretrained_weights=pretrained_weights256) #, device=device256)\n",
        "model4k = get_vit4k(pretrained_weights=pretrained_weights4k)# , device=device4k)\n",
        "\n",
        "### ViT_256 + ViT_4K loaded into HIPT_4K API\n",
        "model = HIPT_4K(pretrained_weights256, pretrained_weights4k, device256, device4k)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HO8jbRTeRAQ4"
      },
      "outputs": [],
      "source": [
        "region = Image.open('./image_demo/image_4k.png')\n",
        "x = eval_transforms()(region).unsqueeze(dim=0)\n",
        "out = model.forward(x)\n",
        "print('Input Shape:', x.shape)\n",
        "print('Output Shape:', out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcBAqUN0OeTV"
      },
      "outputs": [],
      "source": [
        "# loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "# scheduler\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WzIZZayOeWX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()\n",
        "device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSIiQbm1OeZ6"
      },
      "outputs": [],
      "source": [
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "\n",
        "    for data, label in tqdm(train_loader):\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "        output = model(data)\n",
        "        loss = criterion(output, label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        acc = (output.argmax(dim=1) == label).float().mean()\n",
        "        epoch_accuracy += acc / len(train_loader)\n",
        "        epoch_loss += loss / len(train_loader)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        epoch_val_accuracy = 0\n",
        "        epoch_val_loss = 0\n",
        "        for data, label in valid_loader:\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            val_output = model(data)\n",
        "            val_loss = criterion(val_output, label)\n",
        "\n",
        "            acc = (val_output.argmax(dim=1) == label).float().mean()\n",
        "            epoch_val_accuracy += acc / len(valid_loader)\n",
        "            epoch_val_loss += val_loss / len(valid_loader)\n",
        "\n",
        "    writer.add_scalar(\"Loss/train\", epoch_loss, epoch+1)\n",
        "    writer.add_scalar(\"Acc/train\", epoch_accuracy, epoch+1)\n",
        "    writer.add_scalar(\"Loss/Validation\", epoch_val_loss, epoch+1)\n",
        "    writer.add_scalar(\"Acc/Validation\", epoch_val_accuracy, epoch+1)\n",
        "    print(\n",
        "        f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fdn4xEcMMXDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4nVb35yOtqb"
      },
      "outputs": [],
      "source": [
        "#writer.flush()\n",
        "torch.save(model.state_dict(), \"HIPT_v2.pt\")\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmcI7-yNOttq"
      },
      "outputs": [],
      "source": [
        "!tensorboard --logdir=runs "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1oyxsPWrE1u0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}